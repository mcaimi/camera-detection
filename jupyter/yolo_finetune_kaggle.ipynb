{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460414e6-cca9-4e9a-843c-b767886a3c55",
   "metadata": {},
   "source": [
    "# Fine tune YOLOv11 using a public Dataset\n",
    "\n",
    "Fine tuning a YOLOv11 model to recognize new images and new categories. This notebook will outline a general implementation\n",
    "\n",
    "The outcome is to:\n",
    "- Get and save a pretrained YOLO checkpoint from huggingface hub\n",
    "- Download, decompress and load a dataset from Kaggle\n",
    "- Train/Fine-tune the model\n",
    "- Test the new checkpoint\n",
    "- Save the checkpoint for further use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc34cd-ee77-434c-81dd-dd9dba107b74",
   "metadata": {},
   "source": [
    "We'll need a bunch of libraries:\n",
    "- *pandas* in order to deal with structured annotation formats\n",
    "- *pytorch* to deal with training functions and data types\n",
    "- *ultralytics* to actually train the model\n",
    "- *kagglehub* to interface with kaggle repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53665553-a686-4bb4-b3b5-77efe7fcb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision ultralytics pandas numpy huggingface_hub kagglehub python-dotenv\n",
    "%pip list | awk '/torch|ultralytics|pandas|numpy|huggingface|kagglehub|dotenv/ {print $1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacf738-017e-4dc0-89d5-283f58aa6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global setting variables\n",
    "import os\n",
    "HOME_DIR: str = os.getenv(\"HOME\")\n",
    "PRJ_PATH: str = f\"{HOME_DIR}/Work/Sources/camera-detection\"\n",
    "YOLO_MODEL_PATH: str = f\"{PRJ_PATH}/model_checkpoints/Ultralytics/YOLO11/\"\n",
    "YOLO_MODEL_CHECKPOINT: str = \"yolo11x.pt\"\n",
    "YOLO_MODEL_FILE: str = \"yolo11x.yaml\"\n",
    "YOLO_ORIGINAL_MODEL: str = \"/\".join((YOLO_MODEL_PATH, YOLO_MODEL_CHECKPOINT))\n",
    "CONFIG_FILE: str = f\"{PRJ_PATH}/parameters.yaml\"\n",
    "\n",
    "print(f\"Using YOLO Model Original Checkpoint at: {YOLO_ORIGINAL_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kaggle API keys from a dot env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# the .env file should contain the KAGGLE_USERNAME and KAGGLE_KEY environment variables\n",
    "# check whethet they are loaded into the environment..\n",
    "print(f\"Connecting to Kaggle as {os.getenv('KAGGLE_USERNAME')} with API Key {os.getenv('KAGGLE_KEY')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067827ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset from kaggle now\n",
    "DATASET_NAME: str = \"muki2003/yolo-drone-detection-dataset\"\n",
    "TRAINING_DATASET_PATH: str = os.getenv(\"KAGGLEHUB_CACHE\")\n",
    "\n",
    "# get the dataset\n",
    "import kagglehub\n",
    "print(f\"Dataset '{DATASET_NAME}' will be downloaded to {TRAINING_DATASET_PATH}\")\n",
    "dspath: str = kagglehub.dataset_download(DATASET_NAME)\n",
    "print(f\"Dataset available @{dspath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82eb11e-903f-4da4-b966-d99be306624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    from torch import float16, float32\n",
    "    import torch.cuda as tc\n",
    "    import torch.backends.mps as apple_mps\n",
    "    from pathlib import Path\n",
    "    from torchvision.io import read_image\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas import DataFrame, read_table\n",
    "    import ultralytics\n",
    "except Exception as e:\n",
    "    print(f\"Cannot load pytorch: {e}\")\n",
    "\n",
    "try:\n",
    "    from libs.huggingface import pullFromHuggingfaceHub\n",
    "    from libs.parameters import loadConfig, Parameters\n",
    "except Exception as e:\n",
    "    print(f\"Cannot load custom python module: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1e104-2f8b-4556-9c8e-eb879aada5d9",
   "metadata": {},
   "source": [
    "# Define the training dataset and functions to explore it\n",
    "\n",
    "Now we define a custom Dataset class that will hold our training and validation data.\n",
    "The custom class scans the dataset path for images and annotation files and organizes them in a data structure\n",
    "\n",
    "The dataset class is then iterable and returns a data point tuple:\n",
    "- a *torch.Tensor* object holding the image pixel data\n",
    "- a *pandas.DataFrame* object holding the label annotations for that specific image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d649b5b-c677-44be-b6fc-00213ca949bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataset class that holds training data information\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path: str, step: str = \"train\") -> None:\n",
    "        self.image_datapath: Path = Path(path + f\"/{step}\" + \"/images\")\n",
    "        self.labels_datapath: Path = Path(path + f\"/{step}\" + \"/labels\")\n",
    "        \n",
    "        # load objects\n",
    "        self.imgs: list = [f for f in self.image_datapath.glob(\"**/*.jpg\")]\n",
    "        self.labels: list = [f for f in self.labels_datapath.glob(\"**/*.txt\")]\n",
    "\n",
    "        # validate\n",
    "        for fname in self.imgs:\n",
    "            img_id = fname.stem\n",
    "            labelname = self.labels_datapath / f\"{img_id}.txt\"\n",
    "            if labelname not in self.labels:\n",
    "                raise Exception(f\"Missing labels file for image id {img_id}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx) -> (torch.Tensor, DataFrame):\n",
    "        # get image at index 'idx'\n",
    "        image: torch.Tensor = read_image(self.imgs[idx])\n",
    "        # load related annotations\n",
    "        labels: DataFrame = read_table(self.labels_datapath / f\"{self.imgs[idx].stem}.txt\",\n",
    "                                      sep=\" \",\n",
    "                                      header=None)\n",
    "\n",
    "        # return datapoint\n",
    "        return (image, labels)\n",
    "\n",
    "# load labels and ids\n",
    "def loadLabels(descriptorFile: str) -> dict:\n",
    "    import yaml\n",
    "    try:\n",
    "        with open(descriptorFile) as yf:\n",
    "            descriptor_contents = yaml.safe_load(yf)\n",
    "    except Exception as e:\n",
    "        print(f\"Caught YAML Exception: {e}\")\n",
    "\n",
    "    return descriptor_contents.get(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78e244-841c-46c1-8586-89878633cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoint visualization\n",
    "def plotDataPoint(img_rgb: torch.Tensor, annotations: DataFrame, labels: dict) -> None:\n",
    "    \"\"\"\n",
    "        plot an image with its relative object bounding boxes in overlay\n",
    "    \"\"\"\n",
    "    # permute channels and get image sizes\n",
    "    img = img_rgb.permute(1,2,0)\n",
    "    img_h, img_w, channels = img.shape\n",
    "\n",
    "    # plot image\n",
    "    plt.title(f\"Dataset Point: {len(annotations)} objects\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # calculate bounding boxes\n",
    "    axes = plt.gca()\n",
    "    for annotation in range(len(annotations)):\n",
    "        label, center_x, center_y, bounding_w, bounding_h = annotations.loc[annotation, :].values.flatten().tolist()\n",
    "        print(f\"Bounding Box: {label}, {center_x}, {center_y}, {bounding_w}, {bounding_h}\")\n",
    "\n",
    "        # scale coordinates - BOX CENTER\n",
    "        cx, cy = center_x * img_w, center_y * img_h\n",
    "        # scale coordinates - BOX DIMENSIONS\n",
    "        bw, bh = (bounding_w * img_w), (bounding_h * img_h)\n",
    "\n",
    "        # add bounding box\n",
    "        from matplotlib.patches import Rectangle\n",
    "        axes.add_patch(Rectangle((cx - bw/2, cy - bh/2), bw, bh, color=\"white\", fill=None))\n",
    "        # add label\n",
    "        ltext = labels.get(label)\n",
    "        lpos = (cx - bw/2, cy - bh/2 - 10)\n",
    "        axes.text(lpos[0], lpos[1], ltext, color=\"white\", fontsize=12)\n",
    "\n",
    "    # show datapoint\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea98485-b8fb-4819-afc8-200703321a15",
   "metadata": {},
   "source": [
    "# Dataset test\n",
    "\n",
    "A datapoint is a tuple containing:\n",
    "- A torch.Tensor\n",
    "- A pandas DataFrame\n",
    "\n",
    "The tensor represents the image. Its shape is by default in the format (C,H,W):\n",
    "- C: number of channels\n",
    "- H: image height\n",
    "- W: image width\n",
    "\n",
    "To plot the image with pyplot we need to permute channels from (C,H,W) to (H,W,C)\n",
    "\n",
    "The Dataframe holds information about labels, in the format that is needed by the Ultralytics libs to train the YOLO model.\n",
    "The file is structured like this:\n",
    "\n",
    "- One bounding box per row\n",
    "- Each Row contains:\n",
    "  - The Label Class (int)\n",
    "  - The coordinates of the center of the bounding box (float) relative to the image size\n",
    "  - The sizes of the bounding box relative to the image size and the bounding box center (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813522a-759f-4fa5-be53-1af12ceb7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path information\n",
    "dataset_name: str = \"drone_dataset\"\n",
    "dataset_path: str = f\"{dspath}/{dataset_name}\"\n",
    "ds_descriptor_file: str = dataset_path + \"/data.yaml\"\n",
    "\n",
    "# load dataset\n",
    "training_dataset = CustomDataset(dataset_path, step=\"train\")\n",
    "validation_dataset = CustomDataset(dataset_path, step=\"valid\")\n",
    "print(f\"Training Dataset loaded, contains {len(training_dataset)} images.\")\n",
    "print(f\"Validation Dataset loaded, contains {len(validation_dataset)} images.\")\n",
    "\n",
    "# load labels\n",
    "label_dict: dict = loadLabels(ds_descriptor_file)\n",
    "\n",
    "# get a data point from data set\n",
    "try:\n",
    "    datapoint: tuple = training_dataset[0]\n",
    "    \n",
    "    # image from dataset\n",
    "    img: torch.Tensor = datapoint[0]\n",
    "    \n",
    "    # datapoint annotations\n",
    "    annotations: DataFrame = datapoint[1]\n",
    "\n",
    "    # plot datapoint\n",
    "    plotDataPoint(img, annotations, label_dict)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac27ee-97a6-4821-81d1-c7474d9c9bea",
   "metadata": {},
   "source": [
    "# Hardware detection\n",
    "\n",
    "Now we proceed to determine which hardware we can use for training the model.\n",
    "Currently (and based on what pytorch supports), these backends are autodetected:\n",
    "\n",
    "- Plain CPU, no acceleration\n",
    "- Apple Metal Performance Shaders (mps)\n",
    "- Nvidia CUDA (or any accelerator labelled as 'cuda' by pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422405cf-d8e3-420e-8fd5-4b2584c2ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect accelerator\n",
    "def detectAccelerator() -> (str, torch.dtype):\n",
    "    accelerator = \"cpu\"\n",
    "    dtype = float32\n",
    "    \n",
    "    # ensure the apple mps backend is loaded and hardware initialized\n",
    "    if apple_mps.is_available():\n",
    "        print(\"Apple Metal Performance Shaders Available\")\n",
    "        accelerator = \"mps\"\n",
    "        dtype = float16\n",
    "    # check for cuda\n",
    "    elif tc.is_available():\n",
    "        print(\"CUDA Accelerator Available\")\n",
    "        accelerator = \"cuda\"\n",
    "        dtype = float16\n",
    "        !nvidia-smi\n",
    "\n",
    "    # return\n",
    "    return (accelerator, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b736a-e47e-45cc-80e9-24f338f9a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parameters from config file\n",
    "import yaml\n",
    "try:\n",
    "    parms = loadConfig(CONFIG_FILE)\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"Error while loading config parameters {e}\")\n",
    "\n",
    "# analyze detected features\n",
    "def detectedObjects(inferenceOutput):\n",
    "    for o in inferenceOutput:\n",
    "        object_classes = o.names\n",
    "        for obj in o.boxes:\n",
    "            if type(obj.xyxy) is torch.Tensor:\n",
    "                # determine object coordinates\n",
    "                bbox = obj.xyxy.cpu().type(torch.int32).numpy()\n",
    "                x1, y1, x2, y2 = bbox[0]\n",
    "\n",
    "                # determine object classification label and confidence score\n",
    "                class_label = object_classes[int(obj.cls)]\n",
    "                confidence = float(obj.conf)\n",
    "                # print detected object class\n",
    "                print(f\"{class_label} - {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49afae-3c45-475b-ab3b-0fd322e929fc",
   "metadata": {},
   "source": [
    "# Download the pretrained YOLO checkpoint we want to finetune\n",
    "\n",
    "If the checkpoint is not already present on the filesystem, pull it from huggingface\n",
    "Then load the pretrained weights and move the model to the selected accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08781c-618e-4696-98d4-a8f0ca1c1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "JOB = \"detect\"\n",
    "RUN_NAME = \"train\"\n",
    "CHECKPOINT = \"last.pt\"\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "IMG_SIZE = 640\n",
    "BATCH = 2\n",
    "OPTIMIZER = \"AdamW\"\n",
    "AUGMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647a43b-f275-4836-a3fc-cd6f204a4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect accelerator\n",
    "accelerator, dtype = detectAccelerator()\n",
    "\n",
    "# make sure the checkpoint is on the filesystem\n",
    "latest_checkpoint: str = f\"runs/{JOB}/{RUN_NAME}/weights/{CHECKPOINT}\"\n",
    "if os.path.exists(latest_checkpoint):\n",
    "    print(f\"Loading checkpoint {latest_checkpoint}...\")\n",
    "    yolo_model = ultralytics.YOLO(YOLO_MODEL_FILE).load(latest_checkpoint)\n",
    "    yolo_model.to(accelerator)\n",
    "    resume = True\n",
    "else:\n",
    "    if not os.path.exists(YOLO_ORIGINAL_MODEL):\n",
    "        print(f\"Downloading model from huggingface... {YOLO_ORIGINAL_MODEL}\")\n",
    "        pullFromHuggingfaceHub(parms)\n",
    "    # start from scratch\n",
    "    print(f\"Loading checkpoint {YOLO_ORIGINAL_MODEL}...\")\n",
    "    yolo_model = ultralytics.YOLO(YOLO_MODEL_FILE).load(YOLO_ORIGINAL_MODEL)\n",
    "    yolo_model.to(accelerator)\n",
    "    resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training file\n",
    "TRAINING_CONFIG: str = \"training.yaml\"\n",
    "\n",
    "# prepare training config file\n",
    "training_settings = {\n",
    "    \"path\": f\"{dataset_path}\",\n",
    "    \"train\": f\"{dataset_path}/train\",\n",
    "    \"val\": f\"{dataset_path}/valid\",\n",
    "    \"nc\": 1,\n",
    "    \"names\": [\"drone\"],\n",
    "}\n",
    "\n",
    "print(training_settings)\n",
    "\n",
    "with open(TRAINING_CONFIG, \"w\") as file:\n",
    "    yaml.dump(training_settings, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa633eb-ceee-4fea-833f-1251e356df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training!\n",
    "yolo_model.train(data=TRAINING_CONFIG,\n",
    "                 epochs=1, lr0=LR, imgsz=IMG_SIZE, batch=BATCH,\n",
    "                 resume=resume, optimizer=OPTIMIZER, augment=AUGMENT\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a2302-1f7e-493f-903c-b6a8cfa49bcd",
   "metadata": {},
   "source": [
    "# Training Review\n",
    "\n",
    "At this step, the model has been finetuned and we can measure the outcomes: look into the training run folder for detailed graphs and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f2a5d-e68c-48d5-82ac-df7988156cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "from pathlib import PosixPath\n",
    "results_path: str = max(PosixPath(f\"runs/{JOB}/\").glob(\"train*\"), key=os.path.getmtime)\n",
    "\n",
    "print(f\"Latest training output at: {results_path}\")\n",
    "\n",
    "# print graphs\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Training Results\")\n",
    "plt.subplot(121)\n",
    "plt.imshow(read_image(f\"{results_path}/labels.jpg\").permute(1,2,0))\n",
    "plt.subplot(122)\n",
    "plt.imshow(read_image(f\"{results_path}/results.png\").permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43653d-0f26-453c-a2e2-b1d44a5c7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference\n",
    "prediction = yolo_model(dataset_path + \"/valid/images/0001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323bd41-0cee-4740-9f4e-bd26f01a76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detectedObjects(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6deb51-bc6a-425c-af1b-050a7c8456c8",
   "metadata": {},
   "source": [
    "# Save finetuned model and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0cf2b-0ad8-4a77-97b0-66f761b08455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert checkpoint?\n",
    "CONVERT_ONNX = True\n",
    "if CONVERT_ONNX:\n",
    "    yolo_model.export(format=\"onnx\")\n",
    "\n",
    "# export model to data path\n",
    "import shutil\n",
    "shutil.copy(latest_checkpoint, YOLO_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
