{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311d945e-3350-412e-8b66-780ee31f814b",
   "metadata": {},
   "source": [
    "# Try Finetuned YOLO, ONNX Inference\n",
    "\n",
    "Try inferencing with a finetuned YOLO checkpoint, using the ONNX runtime to serve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28846761-e8dd-47e7-9b51-231f1c083ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q opencv-python onnx onnxruntime torch requests\n",
    "%pip list | awk '/opencv|onnx|onnxruntime|torch|requests/ {print $1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bee445-6ef2-42cc-9117-f120068b0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import os\n",
    "from pathlib import PosixPath\n",
    "import onnxruntime as rt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6ebc4-5617-4e2e-8557-0825a01ec113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare global setting variables\n",
    "HOME_DIR: str = os.getenv(\"HOME\")\n",
    "PRJ_PATH: str = f\"{HOME_DIR}/camera-detection\"\n",
    "TEST_IMG: str = \"aircraft_test.jpg\"\n",
    "LABEL_FILE: str = \"labels.yaml\"\n",
    "\n",
    "# search onnx checkpoint\n",
    "onnx_ckpt = max(PosixPath(PRJ_PATH).rglob(\"*.onnx\"), key=os.path.getmtime)\n",
    "print(f\"Using YOLO Model Original Checkpoint at: ONNX - {onnx_ckpt}\")\n",
    "\n",
    "# load labels\n",
    "import yaml\n",
    "with open(LABEL_FILE, \"r\") as f:\n",
    "    labels = yaml.safe_load(f).get(\"names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ab8f8-91d9-4560-bcfe-539cf1c9c868",
   "metadata": {},
   "source": [
    "## First thing, let's try inference locally by loading the ONNX checkpoint directly.\n",
    "\n",
    "This will make sure the model works as expected in a controlled environment (e.g. a jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571690b1-8d74-4972-b45a-28ba1545adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint from disk\n",
    "onnx_model = rt.InferenceSession(onnx_ckpt, providers=rt.get_available_providers())\n",
    "\n",
    "# get inputs and outputs of the onnx model\n",
    "onnx_input = onnx_model.get_inputs()[0]\n",
    "onnx_output = onnx_model.get_outputs()[0]\n",
    "input_name = onnx_input.name\n",
    "output_name = onnx_output.name\n",
    "\n",
    "print(f\"ONNX Model:\\n Input: {input_name}, shape: {onnx_input.shape}\\n Output: {output_name}, shape: {onnx_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904277c-700e-427e-8bfb-43cbcbbdf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoint visualization\n",
    "def plot(img_rgb: torch.Tensor, inferenceOutput, confidences, classes, labels) -> None:\n",
    "    \"\"\"\n",
    "        plot an image with its relative object bounding boxes in overlay\n",
    "    \"\"\"\n",
    "    # permute channels and get image sizes\n",
    "    img = img_rgb.permute(2, 1, 0)\n",
    "    img_h, img_w, channels = img.shape\n",
    "\n",
    "    # plot image\n",
    "    plt.title(f\"Dataset Point: {len(inferenceOutput)} objects\")\n",
    "    plt.imshow(img_rgb)\n",
    "\n",
    "    # calculate bounding boxes\n",
    "    axes = plt.gca()\n",
    "    for i,o in enumerate(inferenceOutput):\n",
    "        c = confidences[i]\n",
    "        cl = classes[i]\n",
    "\n",
    "        x1, y1, x2, y2 = o\n",
    "        #label = object_classes[int(obj.cls)]\n",
    "        print(f\"Bounding Box: {x1}, {y1}, {x2}, {y2}\")\n",
    "\n",
    "        # add bounding box\n",
    "        from matplotlib.patches import Rectangle\n",
    "        axes.add_patch(Rectangle((x1,y1), x2,y2, color=\"white\", fill=None))\n",
    "        # add label\n",
    "        ltext = f\"{c} - {labels.get(cl)}\"\n",
    "        lpos = (x1, y1 - 10)\n",
    "        axes.text(lpos[0], lpos[1], ltext, color=\"white\", fontsize=12)\n",
    "\n",
    "    # show datapoint\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c6c16-b48d-4c5c-b3a9-dfdb06c0abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test image from filesystem\n",
    "test_img = cv2.imread(TEST_IMG)\n",
    "print(f\"Loaded image {TEST_IMG} to memory. Image Size: {test_img.size}@({test_img.shape})\")\n",
    "\n",
    "# resize if necessary\n",
    "onnx_input_img = test_img.copy()\n",
    "N, C, W, H = onnx_input.shape\n",
    "iH, iW, iC = test_img.shape\n",
    "if (H != iH) or (W != iW):\n",
    "    print(f\"Resizing image to {W}x{H} - Current size is {iW}x{iH}\")\n",
    "    resized_img = cv2.resize(onnx_input_img, (W, H))\n",
    "else:\n",
    "    resized_img = onnx_input_img\n",
    "\n",
    "# run inference\n",
    "resized_img = np.transpose(resized_img, (2, 0, 1)).astype(np.float32) / 255.0\n",
    "resized_img = np.expand_dims(resized_img, axis=0)\n",
    "onnx_output = onnx_model.run(None, {input_name: resized_img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e8c21-de54-48b9-b0c4-9e7c69741e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "CONF_THRESHOLD = 0.55\n",
    "predictions = onnx_output[0][0]\n",
    "boxes_data = predictions[0:4, :]\n",
    "scores_data = predictions[4:, :]\n",
    "\n",
    "boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "for i in range(predictions.shape[1]):\n",
    "    class_scores = 1 / (1 + np.exp(-scores_data[:, i]))\n",
    "    max_score = np.max(class_scores)\n",
    "    class_id = np.argmax(class_scores)\n",
    "    cx, cy, w_box, h_box = boxes_data[:, i]\n",
    "\n",
    "    if max_score > CONF_THRESHOLD:\n",
    "        left = int((cx - w_box / 2) * iW / W)\n",
    "        top = int((cy - h_box / 2) * iH / H)\n",
    "        w_px = int(w_box * iW / W)\n",
    "        h_px = int(h_box * iH / H)\n",
    "\n",
    "        boxes.append([left, top, w_px, h_px])\n",
    "        confidences.append(float(max_score))\n",
    "        class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f25fac-0348-48d8-9493-bda8c551b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes, confidences, class_ids)\n",
    "plot(torch.from_numpy(test_img), boxes, confidences, class_ids, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83509ad-169e-4b39-a8d0-944d15ed2ff6",
   "metadata": {},
   "source": [
    "## Now let's call the remote inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb585ca-62df-4408-b04a-02e2e2e0a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint\n",
    "DEPLOYED_MODEL: str = \"yolo-military-finetuned\"\n",
    "INFERENCE_SERVICE_ENDPOINT: str = \"https://yolo-military-finetuned.yolo-finetune.svc.cluster.local\"\n",
    "INFER_URL: str = f\"{INFERENCE_SERVICE_ENDPOINT}/v2/models/{DEPLOYED_MODEL}/infer\"\n",
    "\n",
    "# make request\n",
    "def rest_request(infer_url, data):\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    json_data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"images\",\n",
    "                \"shape\": [1, 3, 640, 640],\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": data,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(infer_url, json=json_data, verify=True)\n",
    "    print(response)\n",
    "    response_dict = response.json()\n",
    "    print(response_dict)\n",
    "    return response_dict['outputs'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a6b9a-5f5b-4166-81fe-f75deadb5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the service\n",
    "result = rest_request(INFER_URL, resized_img.tolist())\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
